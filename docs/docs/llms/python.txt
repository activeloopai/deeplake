Deep Lake Reference (Python)

# Python Reference

Deep Lake is a multi-modal AI database with TQL (Tensor Query Language) for vector similarity search, text search, and complex data operations across cloud storage. It provides native support for embeddings, images, text, and other AI data types with efficient indexing and cross-cloud querying capabilities.

## Initialization

### Creating a client

```python
import deeplake

# Create a new dataset
ds = deeplake.create("s3://bucket/path")  # Cloud storage
ds = deeplake.create("path/to/dataset")   # Local path
ds = deeplake.create("tmp://dataset")     # Temporary dataset
```

### Opening datasets

```python
# Read-write access
ds = deeplake.open("s3://bucket/path")

# Read-only access
ds = deeplake.open_read_only("s3://bucket/path")

# Copy schema from existing dataset
ds = deeplake.like(source_ds, "new/path")
```

### Importing data

```python
# Import from Parquet
ds = deeplake.from_parquet("file.parquet", "output/path")

# Import from CSV
ds = deeplake.from_csv("file.csv", "output/path")
```

## Dataset Operations

### Dataset Class

The main class providing full read-write access.

#### Column Management

```python
# Add new column
ds.add_column("column_name", "float32")
ds.add_column("images", deeplake.types.Image())
ds.add_column("embeddings", deeplake.types.Embedding(768))

# Remove column
ds.remove_column("column_name")

# Rename column
ds.rename_column("old_name", "new_name")
```

#### Data Operations

```python
# Append single sample
ds.append([{
    "images": image_array,
    "labels": "cat",
    "embeddings": embedding_vector
}])

# Append multiple samples (batch)
ds.append({
    "images": [img1, img2, img3],
    "labels": ["cat", "dog", "bird"],
    "embeddings": [emb1, emb2, emb3]
})

# Extend from another dataset
ds.extend(other_dataset)

# Summary information
ds.summary()
```

#### Version Control

```python
# Commit changes
ds.commit("Commit message")
ds.commit_async("Commit message")  # Asynchronous

# Create branch
ds.branch("branch_name")

# Create tag
ds.tag("tag_name")

# Access branches
branch = ds.branches["branch_name"]
branch_ds = branch.open()

# Access tags
tag = ds.tags["tag_name"]
tag_ds = tag.open()

# View history
for version in ds.history:
    print(f"Version {version.id}: {version.message}")

# Get specific version
version = ds.history[version_id]
old_ds = version.open()
```

#### Remote Operations

```python
# Pull changes from remote
ds.pull()
ds.pull_async()  # Asynchronous

# Push changes to remote
ds.push()
ds.push_async()  # Asynchronous

# Refresh dataset
ds.refresh()
ds.refresh_async()  # Asynchronous

# Merge branches
ds.merge("source_branch")
```

#### Dataset Properties

```python
# Metadata
ds.metadata["key"] = "value"
metadata = ds.metadata

# Schema
schema = ds.schema

# Version info
current_version = ds.version
created_time = ds.created_time
current_branch = ds.current_branch

# Description
ds.description = "Dataset description"
description = ds.description

# ID and name
dataset_id = ds.id
dataset_name = ds.name
```

#### ML Framework Integration

```python
# PyTorch DataLoader
from torch.utils.data import DataLoader
loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)

# TensorFlow dataset
tf_dataset = ds.tensorflow()
```

#### Export

```python
# Export to CSV
ds.to_csv("output.csv")

# Batches iterator
for batch in ds.batches(batch_size=100):
    # Process batch
    pass
```

### ReadOnlyDataset Class

Read-only version of Dataset. Cannot modify data but provides access to all data and metadata.

```python
ds = deeplake.open_read_only("path/to/dataset")

# All read operations available
data = ds["column_name"][0:100]
schema = ds.schema
summary = ds.summary()

# ML integrations available
loader = DataLoader(ds.pytorch(), batch_size=32)
tf_dataset = ds.tensorflow()
```

### DatasetView Class

Read-only view of query results.

```python
# Create view from query
view = ds.query("SELECT * WHERE label = 'cat'")

# All read operations available
data = view["column_name"][0:100]
schema = view.schema

# Chain queries
filtered_view = view.query("SELECT * WHERE confidence > 0.9")

# ML integrations available
loader = DataLoader(view.pytorch(), batch_size=32)
tf_dataset = view.tensorflow()

# Export
view.to_csv("filtered_results.csv")
```

## Column Operations

### Column Class

Full read-write access to column data.

```python
# Get column
column = ds["column_name"]

# Access data
data = column[0]           # Single sample
data = column[0:100]       # Slice
data = column[:]           # All data

# Set data
column[0] = new_value
column[0:10] = new_values

# Column properties
column_name = column.name
column_dtype = column.dtype
column_metadata = column.metadata

# Indexing
column.create_index("embedding")     # Vector index
column.create_index("inverted")      # Text search index
column.create_index("btree")         # Numeric index

# Drop index
column.drop_index("embedding")

# Check indexes
indexes = column.indexes

# Async operations
future = column.get_async(0)
data = future.result()

future = column.set_async(0, value)
future.result()

# Get raw bytes
bytes_data = column.get_bytes(0)
future = column.get_bytes_async(0)
bytes_data = future.result()
```

### ColumnView Class

Read-only access to column data.

```python
column = view["column_name"]

# Read operations only
data = column[0]
data = column[0:100]
column_name = column.name
column_dtype = column.dtype
column_metadata = column.metadata
indexes = column.indexes

# Async read operations
future = column.get_async(0)
data = future.result()

future = column.get_bytes_async(0)
bytes_data = future.result()
```

## Data Types

### Basic Types

```python
# Numeric types
"int8", "int16", "int32", "int64"
"uint8", "uint16", "uint32", "uint64"
"float16", "float32", "float64"
"bool"

# Text
"text"

# Usage
ds.add_column("age", "int32")
ds.add_column("score", "float32")
ds.add_column("name", "text")
ds.add_column("is_valid", "bool")
```

### AI-Optimized Types

```python
from deeplake import types

# Image type
ds.add_column("images", types.Image())
ds.add_column("images", types.Image(sample_compression="jpeg"))
ds.add_column("images", types.Image(dtype="uint8"))

# Embedding type (for vector search)
ds.add_column("embeddings", types.Embedding(768))
ds.add_column("embeddings", types.Embedding(
    size=768,
    dtype="float32",
    index_type=types.EmbeddingIndex(types.Clustered)
))

# Text type (with search index)
ds.add_column("text", types.Text())
ds.add_column("text", types.Text(index_type=types.BM25))
ds.add_column("text", types.Text(index_type=types.Inverted))
ds.add_column("text", types.Text(index_type=types.Exact))

# Audio type
ds.add_column("audio", types.Audio())
ds.add_column("audio", types.Audio(sample_compression="mp3"))
ds.add_column("audio", types.Audio(sample_compression="wav"))

# Video type
ds.add_column("videos", types.Video())
ds.add_column("videos", types.Video(sample_compression="mp4"))

# Medical imaging
ds.add_column("medical", types.Medical(compression="dcm"))  # DICOM
ds.add_column("medical", types.Medical(compression="nii"))  # NIfTI

# 3D Mesh
ds.add_column("meshes", types.Mesh())

# Computer Vision
ds.add_column("boxes", types.BoundingBox())
ds.add_column("masks", types.SegmentMask(sample_compression="lz4"))

# Classification
ds.add_column("labels", types.ClassLabel(names=["cat", "dog", "bird"]))
ds.add_column("labels", types.ClassLabel("int32"))  # Numeric labels

# Custom arrays
ds.add_column("features", types.Array("float32", (128,)))
ds.add_column("matrix", types.Array("int32", (10, 10)))

# Dict type (for nested structures)
ds.add_column("metadata", types.Dict({
    "timestamp": "int64",
    "location": {
        "lat": "float32",
        "lon": "float32"
    }
}))

# Sequence type
ds.add_column("sequences", types.Sequence("float32"))
```

## Index Types

### Text Indexes

```python
from deeplake import types

# BM25 - Full-text search with BM25 similarity scoring
ds.add_column("text", types.Text(index_type=types.BM25))

# Inverted - Keyword-based text search
ds.add_column("text", types.Text(index_type=types.Inverted))

# Exact - Exact text matching
ds.add_column("text", types.Text(index_type=types.Exact))
```

### Embedding Indexes

```python
from deeplake import types

# Clustered - Default clustering-based embedding search
ds.add_column("embeddings", types.Embedding(
    768,
    index_type=types.EmbeddingIndex(types.Clustered)
))

# ClusteredQuantized - Memory-efficient quantized embedding search
ds.add_column("embeddings", types.Embedding(
    768,
    index_type=types.EmbeddingIndex(types.ClusteredQuantized)
))
```

### Numeric Indexes

```python
# BTree - Numeric range queries
column.create_index("btree")

# Hash - Exact value lookups
column.create_index("hash")
```

## Query Execution

### Synchronous Queries

```python
# Query using module function
results = deeplake.query("SELECT * FROM dataset WHERE condition")

# Query using dataset instance (no FROM needed)
results = ds.query("SELECT * WHERE label = 'cat'")

# Query on view
results = view.query("SELECT * WHERE confidence > 0.9")
```

### Asynchronous Queries

```python
# Async query using module function
future = deeplake.query_async("SELECT * FROM dataset WHERE condition")
results = future.result()
is_done = future.is_completed()

# Async query using dataset instance
future = ds.query_async("SELECT * WHERE label = 'cat'")
results = future.result()
```

### Prepared Queries

```python
# Prepare query for reuse
executor = deeplake.prepare_query("SELECT * FROM dataset WHERE id = $1")
# Or with dataset
executor = ds.prepare_query("SELECT * WHERE id = $1")

# Get query string
query_string = executor.get_query_string()

# Run with parameters
results = executor.run_single({"id": 123})

# Run batch
results = executor.run_batch([{"id": 1}, {"id": 2}, {"id": 3}])

# Async operations
future = executor.run_single_async({"id": 123})
results = future.result()

future = executor.run_batch_async([{"id": 1}, {"id": 2}])
results = future.result()
```

### Query Explanation

```python
# Explain query execution
explanation = deeplake.explain_query("SELECT * FROM dataset WHERE condition")
# Or with dataset
explanation = ds.explain_query("SELECT * WHERE condition")

# Get explanation as dictionary
plan = explanation.to_dict()

# String representation
print(explanation)
```

### Working with Query Results

```python
# Iterate through results
for item in results:
    image = item["images"]
    label = item["labels"]
    embedding = item["embeddings"]

# Direct column access (faster)
images = results["images"][0:100]
labels = results["labels"][:]
embeddings = results["embeddings"][:]

# Get length
num_results = len(results)

# Convert to PyTorch/TensorFlow
loader = DataLoader(results.pytorch(), batch_size=32)
tf_dataset = results.tensorflow()

# Export results
results.to_csv("query_results.csv")
```

## Schema Management

### Schema Operations

```python
# Get schema
schema = ds.schema

# Access column definitions
column_def = schema["column_name"]
column_dtype = column_def.dtype
column_name = column_def.name

# Iterate columns
for col_name in schema:
    col_def = schema[col_name]
    print(f"{col_name}: {col_def.dtype}")

# Check if column exists
if "column_name" in schema:
    print("Column exists")

# Get number of columns
num_columns = len(schema)
```

### SchemaView (Read-only)

```python
schema = view.schema  # Read-only schema

# All read operations available
column_def = schema["column_name"]
for col_name in schema:
    pass
```

## Metadata Management

### Dataset Metadata

```python
# Set metadata
ds.metadata["author"] = "John Doe"
ds.metadata["version"] = "1.0.0"
ds.metadata["description"] = "My dataset"

# Get metadata
author = ds.metadata["author"]
all_metadata = ds.metadata

# Update metadata
ds.metadata.update({"updated": True})
```

### Column Metadata

```python
# Set column metadata
ds["column_name"].metadata["source"] = "external"
ds["column_name"].metadata["format"] = "RGB"

# Get column metadata
metadata = ds["column_name"].metadata
source = metadata["source"]
```

## Client Operations

### Client Class

Create and manage Deep Lake client for operations:

```python
from deeplake import Client

# Create client with authentication
client = Client(token="your_token")

# Or use default (reads from ACTIVELOOP_TOKEN env var)
client = Client()
```

### Utility Functions

#### Dataset Utilities

```python
# Check if dataset exists
exists = deeplake.exists("path/to/dataset")
exists_async = deeplake.exists_async("path/to/dataset")  # Async

# Delete dataset
deeplake.delete("path/to/dataset")
deeplake.delete_async("path/to/dataset")  # Async

# Copy dataset
deeplake.copy("source/path", "dest/path")
```

#### Data Import Functions

```python
# Import from COCO format
ds = deeplake.from_coco(
    annotation_file="annotations.json",
    image_dir="images/",
    dataset_path="output/path"
)

# Import from Parquet
ds = deeplake.from_parquet("data.parquet", "output/path")

# Import from CSV
ds = deeplake.from_csv("data.csv", "output/path")

# Create dataset like another
source_ds = deeplake.open("source/path")
ds = deeplake.like(source_ds, "new/path")  # Copies schema only
```

## Version Control Operations

### Version Class

Access and manage dataset versions:

```python
# Get current version
current_version = ds.version
version_id = ds.version.id

# Access version history
for version in ds.history:
    print(f"Version {version.id}: {version.message}")
    print(f"Timestamp: {version.timestamp}")

# Get specific version
version = ds.history[version_id]

# Open dataset at specific version
old_ds = version.open()

# Open dataset at specific version asynchronously
future = version.open_async()
old_ds = future.result()

# Access version properties
print(f"ID: {version.id}")
print(f"Message: {version.message}")
print(f"Timestamp: {version.timestamp}")
print(f"Client Timestamp: {version.client_timestamp}")
```

### Branch Operations

Create and manage branches:

```python
# Create branch
ds.branch("experimental")

# Access branches
branches = ds.branches

# List branch names
branch_names = list(branches.names())

# Access specific branch
branch = branches["experimental"]

# Open branch dataset
branch_ds = branch.open()

# Open branch dataset asynchronously
future = branch.open_async()
branch_ds = future.result()

# Branch properties
print(f"Name: {branch.name}")
print(f"ID: {branch.id}")
print(f"Base: {branch.base}")
print(f"Timestamp: {branch.timestamp}")

# Rename branch
branch.rename("new_branch_name")

# Delete branch
branch.delete()

# Check current branch
current = ds.current_branch
print(f"Current branch: {current}")

# Merge branches
ds.merge("experimental")  # Merge experimental into current
```

### Tag Operations

Create and manage tags:

```python
# Create tag
ds.tag("v1.0")
ds.tag("production", version=specific_version)  # Tag specific version

# Access tags
tags = ds.tags

# List tag names
tag_names = list(tags.names())

# Access specific tag
tag = tags["v1.0"]

# Open dataset at tag
tagged_ds = tag.open()

# Open dataset at tag asynchronously
future = tag.open_async()
tagged_ds = future.result()

# Tag properties
print(f"Name: {tag.name}")
print(f"ID: {tag.id}")
print(f"Version: {tag.version}")
print(f"Message: {tag.message}")
print(f"Timestamp: {tag.timestamp}")

# Rename tag
tag.rename("v1.0.0")

# Delete tag
tag.delete()
```

### History Operations

Access dataset history:

```python
# Get history object
history = ds.history

# Iterate through versions
for version in history:
    print(f"Version {version.id}: {version.message}")

# Access by index
first_version = history[0]
latest_version = history[-1]

# Access by version ID
version = history[version_id]

# Get length
num_versions = len(history)
```

## Async Operations

### Future Operations

Work with async operations using Future:

```python
# Async dataset operations
future = deeplake.open_async("path/to/dataset")
ds = future.result()  # Block until ready

# Check completion status
if future.is_completed():
    ds = future.result()
else:
    print("Still loading...")

# Use with async/await
async def load_dataset():
    ds = await deeplake.open_async("path/to/dataset")
    return ds

# Async commit
future = ds.commit_async("message")
future.wait()  # Block until commit completes

# Async query
future = ds.query_async("SELECT * WHERE condition")
results = future.result()

# Async pull/push
future = ds.pull_async()
future.wait()

future = ds.push_async()
future.wait()

# Async delete
future = deeplake.delete_async("path/to/dataset")
future.wait()
```

### FutureVoid Operations

Work with void async operations:

```python
# Async commit returns FutureVoid
future = ds.commit_async("message")

# Wait for completion
future.wait()

# Check completion
if future.is_completed():
    print("Commit finished")

# Use with async/await
async def save_changes():
    await ds.commit_async("message")
```

## Row and Row Range Operations

### Row Class

Access individual rows:

```python
# Get row
row = ds[0]

# Access row data
image = row["images"]
label = row["labels"]
embedding = row["embeddings"]

# Iterate row fields
for key, value in row.items():
    print(f"{key}: {value}")
```

### RowRange and RowRangeView

Work with row ranges:

```python
# Get row range
row_range = ds[0:100]

# Access row range view
row_range_view = view[0:50]

# Iterate row range
for row in row_range:
    process_row(row)

# Access by index in range
first_row = row_range[0]
```

## Advanced Features

### Auto-Commit

Enable automatic commits:

```python
# Enable auto-commit
ds.auto_commit_enabled = True

# Now changes are committed automatically
ds.append(data)  # Auto-committed

# Disable auto-commit
ds.auto_commit_enabled = False

# Manual commits required
ds.append(data)
ds.commit("Manual commit")
```

### Indexing Mode

Control indexing behavior:

```python
# Set indexing mode
ds.indexing_mode = deeplake.IndexingMode.MANUAL  # Manual indexing
ds.indexing_mode = deeplake.IndexingMode.AUTOMATIC  # Automatic indexing

# Get current mode
mode = ds.indexing_mode
```

### Credentials Management

Manage cloud credentials:

```python
# Set credentials key
ds.set_creds_key("aws_key", creds={
    "aws_access_key_id": "key",
    "aws_secret_access_key": "secret"
})

# Get credentials key
creds_key = ds.creds_key

# Use credentials in operations
ds = deeplake.open(
    "s3://bucket/dataset",
    creds={
        "aws_access_key_id": "key",
        "aws_secret_access_key": "secret",
        "aws_region": "us-east-1"
    }
)
```

### Dataset Properties

Access dataset properties:

```python
# Basic properties
dataset_id = ds.id
dataset_name = ds.name
description = ds.description
created_time = ds.created_time
current_branch = ds.current_branch
current_version = ds.version

# Set description
ds.description = "My dataset description"

# Get dataset length
num_samples = len(ds)

# Get dataset summary
summary = ds.summary()
print(summary)
```

### Link Operations

Link datasets:

```python
# Link dataset
deeplake.link("source/path", "link/path")

# Link dataset asynchronously
future = deeplake.link_async("source/path", "link/path")
future.wait()
```

## Best Practices

### Efficient Data Loading

```python
# Use batches for large datasets
for batch in ds.batches(batch_size=1000):
    process_batch(batch)

# Use async operations for I/O-bound tasks
future = ds.commit_async("message")
# Do other work
future.result()  # Get result when needed
```

### Memory Management

```python
# Use read-only mode when only reading
ds = deeplake.open_read_only("path")

# Use views to filter data before loading
view = ds.query("SELECT * WHERE date > '2024-01-01'")
data = view["images"][:]  # Only loads filtered data
```

### Indexing Strategy

```python
# Create indexes after adding data
ds.append(data)
ds.commit()

# Create indexes for searchable columns
ds["text"].create_index("inverted")  # For text search
ds["embeddings"].create_index("embedding")  # For vector search
```

## Documentation

For more information, see:

- [Dataset API](https://docs.deeplake.ai/api/dataset/): Complete dataset operations
- [Column API](https://docs.deeplake.ai/api/column/): Column management and indexing
- [Data Types](https://docs.deeplake.ai/api/types/): All supported data types
- [Query API](https://docs.deeplake.ai/api/query/): Query execution and prepared queries
- [Version Control](https://docs.deeplake.ai/api/version_control/): Branches, tags, and versioning
- [Schemas](https://docs.deeplake.ai/api/schemas/): Pre-built schema templates
