replicaCount: 2

image:
  repository: quay.io/activeloopai/pg-deeplake
  tag: "18"
  pullPolicy: IfNotPresent

# Image pull secrets (required for private registries like quay.io).
# In production, use an ExternalSecret to provision regcred-quay.
imagePullSecrets: []
  # - name: regcred-quay

# Service account for IRSA (IAM Roles for Service Accounts).
# Allows S3 access without static AWS credentials.
serviceAccount:
  create: false
  name: ""
  automountServiceAccountToken: true
  annotations: {}
    # eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/pg-deeplake-s3"

deeplake:
  rootPath: "s3://your-bucket/your-prefix/"
  syncIntervalMs: 2000
  memoryLimitMb: 0
  startupJitterMaxSeconds: 0

# Static AWS credentials â€” only for dev/testing.
# For production, use IRSA (serviceAccount.annotations with eks role-arn)
# or an ExternalSecret that provisions these into a K8s Secret.
aws:
  accessKeyId: ""
  secretAccessKey: ""
  sessionToken: ""

postgres:
  user: postgres
  password: ""
  database: postgres
  hostAuthMethod: scram-sha-256

security:
  # Enforce secure chart inputs (recommended for production values files).
  # When enabled, blocks weak inline postgres.password values unless envFromSecret is set.
  enforceSecureDefaults: true
  # When enabled (and enforceSecureDefaults=true), require envFromSecret for DB/AWS credentials.
  requireEnvFromSecret: false

# External Secrets (1Password / AWS Secrets Manager / etc).
# When configured, secrets are provisioned by External Secrets Operator
# instead of the inline Secret template.
# Pattern matches live-environments: each entry becomes an ExternalSecret CRD.
externalSecrets: []
  # - name: pg-deeplake-creds
  #   spec:
  #     refreshInterval: 5m
  #     secretStoreRef:
  #       name: 1password-sdk
  #       kind: ClusterSecretStore
  #     dataFrom:
  #       - extract:
  #           key: pg-deeplake-serverless
  # - name: regcred-quay
  #   spec:
  #     refreshInterval: 5m
  #     secretStoreRef:
  #       name: 1password-sdk-ops
  #       kind: ClusterSecretStore
  #     dataFrom:
  #       - extract:
  #           key: regcred-quay
  #     target:
  #       creationPolicy: Owner
  #       deletionPolicy: Retain
  #       template:
  #         type: kubernetes.io/dockerconfigjson

# Name of an externally-managed Secret to inject as envFrom into pg-deeplake pods.
# Use this when secrets are provisioned by ExternalSecrets or manually.
# The secret should contain: POSTGRES_PASSWORD, AWS_ACCESS_KEY_ID,
# AWS_SECRET_ACCESS_KEY, and optionally AWS_SESSION_TOKEN.
envFromSecret: ""

persistence:
  enabled: true
  storageClass: ""
  size: 10Gi

resources:
  requests:
    cpu: "1"
    memory: "2Gi"
  limits:
    cpu: "4"
    memory: "8Gi"

shmSize: 1Gi

# PostgreSQL tuning parameters (adjust to match resource limits)
postgresConfig:
  sharedBuffers: "512MB"
  workMem: "2GB"
  maintenanceWorkMem: "512MB"
  effectiveCacheSize: "4GB"
  maxConnections: 200
  maxWalSize: "4GB"
  checkpointTimeout: "30min"
  logMinDurationStatement: 1000

# Pod-level security context (matches org standard)
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 999
  fsGroup: 999
  seccompProfile:
    type: RuntimeDefault

# Container-level security context
securityContext:
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false

# Extra labels applied to all resources
labels: {}

# Extra annotations applied to pods
annotations: {}
  # reloader.stakater.com/auto: "true"

# Default scheduling hardening (used when explicit affinity is not provided).
defaultScheduling:
  database:
    enablePodAntiAffinity: true
    topologyKey: kubernetes.io/hostname
    enableTopologySpread: true
    spreadTopologyKey: topology.kubernetes.io/zone
    maxSkew: 1
    whenUnsatisfiable: ScheduleAnyway
  haproxy:
    enablePodAntiAffinity: true
    topologyKey: kubernetes.io/hostname
    enableTopologySpread: true
    spreadTopologyKey: topology.kubernetes.io/zone
    maxSkew: 1
    whenUnsatisfiable: ScheduleAnyway

tls:
  enabled: true
  # Name of a Kubernetes TLS Secret (must contain tls.crt and tls.key).
  # Create with: kubectl create secret tls pg-deeplake-tls --cert=server.crt --key=server.key
  secretName: "pg-deeplake-tls"

haproxy:
  enabled: true
  replicaCount: 2
  image:
    repository: haproxy
    tag: "2.9"
  resources:
    requests:
      cpu: "250m"
      memory: "256Mi"
    limits:
      cpu: "1"
      memory: "512Mi"
  # Connection queue timeout: how long to hold connections while waiting
  # for backends to become available (enables wake-on-connect).
  queueTimeout: "120s"
  stats:
    # Set to "user:password" to enable the stats HTML dashboard.
    # Leave empty to disable the dashboard (prometheus metrics remain available).
    auth: ""
  service:
    type: LoadBalancer
    port: 5432
    statsPort: 8404
    exposeStats: false
  nodeSelector: {}
  tolerations: []
  affinity: {}

redis:
  enabled: true
  url: ""   # Auto-constructed from service name when empty
  image:
    repository: redis
    tag: "7-alpine"
  maxMemory: "512mb"
  # For production HA, use an external Redis cluster (e.g. AWS ElastiCache,
  # Bitnami Redis Sentinel chart). Single-replica Redis is acceptable when
  # the cache is purely a performance optimization and not critical path.
  replicaCount: 1
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "640Mi"
  service:
    port: 6379

autoscaling:
  enabled: true
  minReplicas: 0
  maxReplicas: 8
  idleReplicaCount: 0
  # KEDA scales based on HAProxy's queued connection count (prometheus trigger).
  # This works at 0 DB replicas because HAProxy is always running as the
  # activation proxy. New connections queue at HAProxy -> metric rises ->
  # KEDA scales up -> backends become healthy -> HAProxy forwards.
  connectionQueueThreshold: "1"
  cooldownPeriod: 300    # seconds before scale-down after last trigger
  pollingInterval: 5     # seconds between KEDA metric checks

monitoring:
  alerts:
    enabled: true
    # Extra labels for PrometheusRule (e.g., to match Prometheus selector)
    labels: {}
      # release: kube-prometheus-stack
    # Alert thresholds
    queueDepthThreshold: 10
    highConnectionsThreshold: 150
    slowBackendThresholdSeconds: 5
    # Optional alert ownership/runbook metadata.
    owner: ""
    runbookBaseUrl: ""

nodeSelector: {}
tolerations: []
affinity: {}
